{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Model Code","provenance":[],"authorship_tag":"ABX9TyPUaCmZDodB8mEXR/IYT3xZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"eMiYx1Ra1HZE"},"source":["# You may import additional packages as appropriate\n","from __future__ import print_function\n","from __future__ import division\n","\n","import math\n","import pickle\n","import random\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","from collections import Counter\n","from torch.utils.data import DataLoader, TensorDataset\n","from torch.nn.utils.rnn import pack_padded_sequence, invert_permutation\n","from torch.nn.utils import clip_grad_norm_\n","\n","import matplotlib.pyplot as plt\n","\n","%matplotlib inline\n","plt.rcParams['figure.figsize'] = (10.0, 8.0)\n","assert torch.cuda.is_available(), 'GPU unavailable'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"edn-JDdb1hBU"},"source":["def eval_acc(model, data_loader, num_samples=5):\n","  \n","  with torch.no_grad():\n","    model.eval()\n","    \n","    cond_len = 4  # provide the first 4 tokens (including 'bos')\n","    pred_len = MAX_LEN + 2 - cond_len  # predict the rest\n","    \n","    total = 0\n","    correct = 0\n","    \n","    for batch, data in enumerate(data_loader):\n","      data = [d.cuda() for d in data]\n","      \n","      cond_text = data[0][:, :cond_len]\n","      pred = model.predict(cond_text, pred_len)\n","      \n","      if batch < num_samples:  # show some samples\n","        target_sentence = [target_vocab.index2word[token] for token in data[0][0].tolist()]\n","        pred_sentence = [target_vocab.index2word[token] for token in pred[0].tolist()]\n","        \n","        print(f'=== Sample {batch + 1} ===')\n","        print('target_sentence:', target_sentence)\n","        print('pred_sentence:  ', pred_sentence)\n","      \n","      target_len, sorted_indices = torch.sort(data[1], descending=True)\n","      target_text = data[0].index_select(0, sorted_indices)\n","      pred = pred.index_select(0, sorted_indices)\n","      target_len = target_len.cpu()\n","      \n","      target_text_packed = pack_padded_sequence(\n","        target_text[:, cond_len:], target_len - cond_len,\n","        batch_first=True, enforce_sorted=True)\n","      \n","      pred_packed = pack_padded_sequence(\n","        pred[:, cond_len:], target_len - cond_len,\n","        batch_first=True, enforce_sorted=True)\n","      \n","      total += target_text_packed.data.shape[0]\n","      correct += (pred_packed.data == target_text_packed.data).sum().item()\n","    \n","    acc = 100 * correct / total\n","  \n","  return acc"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QQK5fJWw1BJB"},"source":["def train(model, train_loader, val_loader, num_epochs, d_model, learning_rate=None):\n","  \n","  \n","  num_steps = 0\n","  loss_history = []\n","  for epoch in range(num_epochs):\n","    val_acc = eval_acc(model, val_loader, num_samples=0)\n","    \n","    model.train()\n","    for batch, data in enumerate(train_loader):\n","      num_steps+=1\n","      lr = learning_rate\n","      if learning_rate is None:\n","        lr = d_model**(-0.5) * min(num_steps**(-0.5), num_steps*(4000**(-1.5)))\n","      optimizer = optim.Adam(model.parameters(), lr=lr)\n","      data = [d.cuda() for d in data]\n","      \n","      optimizer.zero_grad()\n","      loss = model(*data)\n","      loss.backward()\n","      clip_grad_norm_(model.parameters(), 1)  # gradient clipping\n","      optimizer.step()\n","      \n","      with torch.no_grad():\n","        loss_history.append(loss.item())\n","        if batch == 0:\n","          print('Train Epoch: {:3} \\t Loss: {:F} \\t Val Acc: {:F}'.format(\n","            epoch, loss.item(), val_acc))\n","  \n","  return model, loss_history"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zLx4KUi-1Ks6"},"source":["torch.manual_seed(0)\n","\n","batch_size = 100\n","num_epochs = 40\n","learning_rate = 0.0001\n","\n","num_blocks = 10\n","d_model = 520\n","num_heads = 10\n","d_ff = 4 * d_model\n","dropout = 0.0\n","\n","target_vocab, train_loader, val_loader = get_data_loader(target_text, batch_size)\n","model = Transformer(num_blocks, target_vocab.size, MAX_LEN + 2, d_model, num_heads, d_ff, dropout)\n","model = model.cuda()\n","\n","model, loss_history = train(model, train_loader, val_loader, num_epochs, d_model, learning_rate=learning_rate)"],"execution_count":null,"outputs":[]}]}